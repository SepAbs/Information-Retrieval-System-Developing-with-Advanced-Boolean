{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project 1\n",
    "Sepehr Abbaspour\n",
    "610398147"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we import 'NLTK' library for tokenisation and achieve stop words list.\n",
    "The two functions needed are called 'wordpunct_tokenize' and 'stopwords', respectively.\n",
    "os library and distance function from Levenshtein library are also imported to design an output text as Google and calculate Levenshtein distance, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nltk\n",
    "os.system('color')\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from Levenshtein import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining preprocessor function which preprocess the terms in the documents and queries.\n",
    "In this function the input string is changed in the way of lowering its letters, elimination of the stop words and asymmetric expansion (changing from plural form to singular form in the specific rules).\n",
    "Tokenisation level is to delete symbolic letters by iterating the full-lowered string.\n",
    "Some variables are defined to be pointed for the next times.\n",
    "After that, the terms are devided and assembled to make String be the list of them and each of them gets ready for a possible changing.\n",
    "The changing contains stop word deletion in which stop words become removed from the list and replaces with null phrase (\"\").\n",
    "This replacements exist to avoid changing the length of the list 'String' while iterating it.\n",
    "After stop words deletion, we check any plural form for each non-stop word term and if its in any plural form, it'll be changed to its singular form according to official plural nouns table rules.\n",
    "At last the null phrases in the list of edited terms become removed and the final list is returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessor(String):\n",
    "    String = wordpunct_tokenize(String.replace(\".\", \"\").lower()) #Case Folding & Tokenisations\n",
    "    null, Consonants, Irregulars, stopWords = \"\", \"qwrtypsdfghjklzxcvbnm\", {\"children\":\"child\", \"feet\":\"foot\", \"teeth\": \"tooth\", \"mice\": \"mouse\", \"people\": \"person\"}, stopwords.words('english')\n",
    "\n",
    "    for Index in range(len(String)):\n",
    "        if String[Index] in stopWords: #Stop Words Elimination\n",
    "            String[Index] = null\n",
    "            \n",
    "        #Asymmetric Expansion\n",
    "        elif String[Index][-3:] == \"ies\" and String[Index][-4] in Consonants:\n",
    "            String[Index] = String[Index][:-3] + \"y\"\n",
    "\n",
    "        elif String[Index][-3:] == \"ves\":\n",
    "            String[Index] = String[Index][:-3] + \"f\"\n",
    "            \n",
    "        elif String[Index][-2:] == \"es\" and (String[Index][-3] in [\"s\", \"x\", \"z\"] or String[Index][-4:-2] in [\"ch, sh\"] or (String[Index][-3] == \"o\" and String[Index][-4] in Consonants)):\n",
    "            String[Index] = String[Index][:-2]\n",
    "        \n",
    "        elif String[Index][-1] == \"s\":\n",
    "            String[Index] = String[Index][:-1]\n",
    "            \n",
    "        elif String[Index][-3:] == \"men\":\n",
    "            String[Index] = String[Index][:-3] + \"man\"\n",
    "        \n",
    "        elif String[Index] in Irregulars.keys():\n",
    "            String[Index] = Irregulars[String[Index]]\n",
    "        \n",
    "    while null in String:\n",
    "            String.remove(null)      \n",
    "    return String"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're gonna contruct handler functions for boolean and also proximity, individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first funtion handles queries include the word 'and'.\n",
    "This function removes the boolean and preprocesses the non-boolean words by the means of the defined function 'preProcessor' and then, check some conditions to return the desired list of document IDs.\n",
    "At first it checks wether the lower boundary of one postings list is equal to the other or not, or the words from input query are both in the documentaries or not. If both of them becomes negative, the function returns empty list as the intersection.\n",
    "Else it iterates the postings list of the first word in the query and append any document ID which is also in the postings list of the second word in the query.\n",
    "This function now handles the input queries which doesn't exist in any documentaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def And(Query):\n",
    "    Query = preProcessor(Query.replace(\" and \", \" \"))\n",
    "    if any(Q not in InvertedIndexStruct for Q in Query):\n",
    "        return []\n",
    "    \n",
    "    ID1, ID2 = InvertedIndexStruct[Query[0]], InvertedIndexStruct[Query[1]]\n",
    "    if ID1[-1] < ID2[0] or ID2[-1] < ID1[0]:\n",
    "        return []\n",
    "\n",
    "    AND = []\n",
    "    for ID in ID1:\n",
    "        if ID in ID2:\n",
    "            AND.append(ID)\n",
    "    return sorted(AND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second funtion handles queries include the word 'not'.\n",
    "This function removes the boolean and preprocesses the non-boolean words by the means of the defined function 'preProcessor' and then, check some conditions to return the desired list of document IDs.\n",
    "At first it checks wether the first word in query which hast the same postings list element with what should achieved from this function is in the documentaries or not. If not, it returns the empty list and ignore the remaining of the query.\n",
    "If it does exist, it iterates the postings list of the first word in the query and append any document ID which doesn't exist in the postings list of the second word in the query which is pinned by 'not' as its prefix.\n",
    "This function now handles the input queries which doesn't exist in any documentaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Not(Query):\n",
    "    Query = preProcessor(Query.replace(\" not \", \" \"))\n",
    "    if Query[0] not in InvertedIndexStruct:\n",
    "        return []\n",
    "    \n",
    "    if Query[1] not in InvertedIndexStruct:\n",
    "        return InvertedIndexStruct[Query[0]]\n",
    "    \n",
    "    NOT = []\n",
    "    ID1, ID2 = InvertedIndexStruct[Query[0]], InvertedIndexStruct[Query[1]]\n",
    "    for ID in ID1:\n",
    "        if ID not in ID2:\n",
    "            NOT.append(ID)\n",
    "    return sorted(NOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third funtion handles queries include the word 'or'.\n",
    "This function removes the boolean and preprocesses the non-boolean words by the means of the defined function 'preProcessor' and then, check some conditions to return the desired list of document IDs.\n",
    "At first it checks wether is there any word in the input query in the documentaries or not. If there's none of them, the function returns empty list and shows that the query is out of the documentaries.\n",
    "If the length of the postings list of any word in the input query is as much as the number of documentaries, it's concludes that the mentioned word appears in all documentaries and it's postings list would be returned as the definition of 'or' operator.\n",
    "If any conditions isn't satisfied, exist, it iterates the words in documentaries and assemble their postings list together.\n",
    "This function now handles the input queries which doesn't exist in any documentaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Or(Query):\n",
    "    Query = preProcessor(Query.replace(\" or \", \" \"))\n",
    "    if all(Q not in InvertedIndexStruct for Q in Query):\n",
    "        return []\n",
    "    \n",
    "    for Q in Query:\n",
    "        if Q in InvertedIndexStruct:\n",
    "            QID = InvertedIndexStruct[Q]\n",
    "            if len(QID) == lengthDocs:\n",
    "                return QID\n",
    "            \n",
    "    OR = []\n",
    "    for Q in Query:\n",
    "        if Q in InvertedIndexStruct:\n",
    "            OR += InvertedIndexStruct[Q]     \n",
    "    return sorted(set(OR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last query handler function is Proximity.\n",
    "This function removes the proximity phrase in ((near/distance)) form and preprocesses the non-boolean words by the means of the defined function 'preProcessor' and then, check some conditions to return the desired list of document IDs.\n",
    "The first condition is same as the first condition in the function 'And' because the words of the queries definitions. \n",
    "If the conditions aren't satisfied, the function collects the document IDs which contain both of the words to calculates their distance and append them to the list called 'PROX', then.\n",
    "The function iterates PROX and opens files whose ID is mentioned by one of the elements of PROX and calculates distance between the desired words. If the distance becomes less or equal the boundary the user entered, the document ID will be appended to a special list called 'Proximity'.\n",
    "At last the list gets sorted and returned.\n",
    "This function now handles the input queries which doesn't exist in any documentaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Proximity(Query):\n",
    "    String = f\" near/{Query.split()[1][-1]} \"\n",
    "    Query = preProcessor(Query.replace(String, \" \"))\n",
    "    if any(Q not in InvertedIndexStruct for Q in Query):\n",
    "        return []\n",
    "    \n",
    "    Q0, Q1 = Query[0], Query[1]\n",
    "    ID1, ID2 = InvertedIndexStruct[Q0], InvertedIndexStruct[Q1]\n",
    "    if ID2[-1] < ID1[0] or ID1[-1] < ID2[0]:\n",
    "        return []\n",
    "\n",
    "    PROX = []\n",
    "    for ID in ID1:\n",
    "        if ID in ID2:\n",
    "            PROX.append(ID)\n",
    "\n",
    "    Proximity = []\n",
    "    for Element in PROX:\n",
    "        File = open(listDocs[Element - 1], \"r\")\n",
    "        Words = preProcessor(File.read())\n",
    "        if abs(Words.index(Q0) - Words.index(Q1)) <= int(String[-2]):\n",
    "            Proximity.append(Element)\n",
    "        File.close()\n",
    "    return sorted(Proximity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there's bigram converter for each non-stop singular words in documentaries for a great '*' included searching.\n",
    "It split the complex query by '*' and make bigram indexes of the word and append them in a list called 'gramList'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biGrams(String):\n",
    "    lessLengthWC, First, Last = len(String) - 1, String[0], String[-1]\n",
    "    if First != \"*\":\n",
    "        gramList = [f\"${First}\"]\n",
    "    else:\n",
    "        gramList = []\n",
    "\n",
    "    String = String.split(\"*\")\n",
    "    for Element in String:\n",
    "        lengthElement = len(Element) - 1\n",
    "        for Index in range(lengthElement):\n",
    "            gramList.append(Element[Index : Index + 2])\n",
    "\n",
    "    if Last != \"*\":\n",
    "        gramList.append(f\"{Last}$\")\n",
    "\n",
    "    return gramList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function is also defined which calls one of the defined query handler functions according to what type of queries the user input.\n",
    "Now it has the able to handle complex '*' included and miss spelled queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main(Query):\n",
    "    Query = Query.split()\n",
    "    for Index in [0, 2]:\n",
    "        Word = Query[Index]\n",
    "        if \"*\" in Word:\n",
    "            setBigrams, listSimilars = set(biGrams(Word)), []\n",
    "            for Vals in listVals:\n",
    "                setVals = set(Vals)\n",
    "                lengthSBV, lengthSVB, Boundary = len(setBigrams - setVals), len(setVals - setBigrams), len(Vals) // 2 + 1 #A hand-made good heuristic boundary to find out how similar the bigram index lists are.\n",
    "                if all(Val in Word for Val in Vals) or all(Val in Vals for Val in Word) or any(length <= Boundary for length in [lengthSBV, lengthSVB]):\n",
    "                    listSimilars.append(listKeys[listVals.index(Vals)])\n",
    "    \n",
    "            if len(listSimilars) > 1: #Just in Case! #Although a basic but good heuristic boundary has been defined, it's good to find the word with the least Levenshtein distance.\n",
    "                listCounter = []\n",
    "                for Key in listSimilars:\n",
    "                    listCounter.append(distance(Word, Key))\n",
    "                Query[Index] = listKeys[listCounter.index(min(listCounter))]\n",
    "                    \n",
    "            else:\n",
    "                Query[Index] = listSimilars[0]\n",
    "                    \n",
    "            print(f\"Hah, Gotcha! Why so nervous and neckless to type '{Query[Index]}' correctly?!\")\n",
    "            print()\n",
    "\n",
    "        elif Word not in BGs:\n",
    "            listCounter = []\n",
    "            for Key in BGs:\n",
    "                if len(Word) <= int(1.25 * len(Key)): #Another hand-made good heuristic boundary!\n",
    "                    listCounter.append(distance(Word, Key))\n",
    "                    \n",
    "            if listCounter != []:\n",
    "                Query[Index] = listKeys[listCounter.index(min(listCounter))]\n",
    "                print(f\"Did you mean: \\033[1;3m{Query[Index]}\\033[0m\")\n",
    "                \n",
    "            else:\n",
    "                print(\"ERROR 404! NOT FOUND\")\n",
    "                print()\n",
    "\n",
    "    Query = \" \".join(Query)\n",
    "    if \"and\" in Query:\n",
    "        return And(Query)\n",
    "    \n",
    "    elif \"not\" in Query:\n",
    "        return Not(Query)\n",
    "                \n",
    "    elif \"or\" in Query:\n",
    "        return Or(Query)\n",
    "    \n",
    "    else:\n",
    "        return Proximity(Query) #The stop words are ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there's the ability of getting text file names (case-insensivity is considered by the instruction '.lower()') from user and then, append the input file name to a list created for retaining all names of the documents.\n",
    "This continues till the user enter \"Ctrl+d\" which causes EOF error but handled by try... except EOFError instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Globals!\n",
    "listDocs, lengthDocs = [], 0\n",
    "while True:\n",
    "    try:\n",
    "        while True:\n",
    "            Document = input(\"Enter your text document: \")\n",
    "            if \".txt\" == Document[-4:].lower() and Document not in listDocs:\n",
    "                break\n",
    "        listDocs.append(Document)\n",
    "        lengthDocs += 1\n",
    "        \n",
    "    except EOFError: #Enter Ctrl+d to exit\n",
    "        print(\"START!\")\n",
    "        print()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we're going to construct the inverted index structure.\n",
    "Select each document by iterating the list of the names of the input text files and then, open the file. Then it preprocesses on the terms of the opened file line by line by iterating the lines of the file and if the line isn't just a new line, the function 'preProcessor' will be called to preprocess whole strings in that line.\n",
    "Right after the line is preprocessed, the list of the included terms in the line concate with the list of terms of the opened text file.\n",
    "This list is used to creating the postings list for each terms of the file and appears in the inverted index structure which has the document IDs of each term exists. If the term is occured in some prior investigated text files, the new document ID is appended to the list of previous document IDs for that term. \n",
    "After doing all above for an opened file, the file become closed.\n",
    "After the file iteration, the inverted index structure will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InvertedIndexStruct, BGs = dict(), dict()\n",
    "for Index in range(lengthDocs):\n",
    "    Lines, File = [], open(listDocs[Index], \"r\")\n",
    "    for Line in File:\n",
    "        if Line != \"\\n\":\n",
    "            Lines += preProcessor(Line)\n",
    "\n",
    "    if Lines != []:\n",
    "        Lines = sorted(Lines)\n",
    "        for Term in Lines:\n",
    "            I = Index + 1\n",
    "            if Term in InvertedIndexStruct and I not in InvertedIndexStruct[Term]:\n",
    "                InvertedIndexStruct[Term].append(I)\n",
    "            else:\n",
    "                InvertedIndexStruct[Term] = [I]\n",
    "                \n",
    "            if Term not in BGs:\n",
    "                 BGs[Term] = biGrams(Term) \n",
    "    File.close()\n",
    "    \n",
    "listKeys, listVals = list(BGs.keys()), list(BGs.values())\n",
    "print(\"Term-Document Inverted Index Structure:\")\n",
    "print()\n",
    "print(InvertedIndexStruct)\n",
    "print()\n",
    "print(\"K-Gram Index Structure:\")\n",
    "print()\n",
    "print(BGs)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query time! This block begins by getting boolean, proximity ('NEAR/' form), complex (wild-card) or miss-spelled queries  query and nothing else (case-insensivity is considered by the instruction '.lower()').\n",
    "Then it elicit the target terms by calling the function 'Main' which calls the related function based on which type of query is input.\n",
    "While user answer 'N' to the question whether she wants to terminate the program, the loop continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    Query = input(\"What are you looking for? \").lower()\n",
    "    BooList = [\" not \", \" and \", \" or \", \" near/\", \"*\"] #As the default, promixity has the least priority. Star is added!\n",
    "    while all(BOO not in Query for BOO in BooList):\n",
    "        Query = input(\"What are you looking for? Boolean or proximity please. \").lower()\n",
    "\n",
    "    print()\n",
    "    print(Main(Query)) #Main is ready to be called.\n",
    "    print()\n",
    "    \n",
    "    End = input(\"Wanna end this? <Y, N> \")\n",
    "    while End not in [\"Y\", \"y\", \"N\", \"n\"]:\n",
    "        End = input(\"DON'T WASTE MY TIME! You wanna end this? <Y, N> \")\n",
    "    if End in \"Yy\":\n",
    "        print(\"Good-bye\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Goodluck!\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program is in an optimal form due to the pointed conditions in the constructions of the defined query handler functions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
